{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install mplfinance"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GG6MEfa8iZYC",
        "outputId": "380852f1-1cca-4136-b71a-fba085d4e578"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mplfinance\n",
            "  Downloading mplfinance-0.12.10b0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from mplfinance) (3.7.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from mplfinance) (1.5.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mplfinance) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mplfinance) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mplfinance) (4.42.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mplfinance) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mplfinance) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mplfinance) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mplfinance) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mplfinance) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mplfinance) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->mplfinance) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->mplfinance) (1.16.0)\n",
            "Installing collected packages: mplfinance\n",
            "Successfully installed mplfinance-0.12.10b0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install keras\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3U60JVAeaJY",
        "outputId": "162a9e9a-c119-41d3-9f3d-84118e034fd5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.13.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# File: stock_prediction.py\n",
        "# Authors: Cheong Koo and Bao Vo\n",
        "# Date: 14/07/2021(v1); 19/07/2021 (v2); 25/07/2023 (v3)\n",
        "\n",
        "# Code modified from:\n",
        "# Title: Predicting Stock Prices with Python\n",
        "# Youtuble link: https://www.youtube.com/watch?v=PuZY9q-aKLw\n",
        "# By: NeuralNine\n",
        "\n",
        "# Need to install the following:\n",
        "# pip install numpy\n",
        "# pip install matplotlib\n",
        "# pip install pandas\n",
        "# pip install tensorflow\n",
        "# pip install scikit-learn\n",
        "# pip install pandas-datareader\n",
        "# pip install yfinance\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import pandas_datareader as web\n",
        "import datetime as dt\n",
        "import tensorflow as tf\n",
        "import yfinance as yf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, LSTM, InputLayer, Bidirectional, GRU\n",
        "import os\n",
        "import pickle\n",
        "import joblib\n",
        "import mplfinance as fplt\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "#------------------------------------------------------------------------------\n",
        "# Load Data\n",
        "## TO DO:\n",
        "# 1) Check if data has been saved before.\n",
        "# If so, load the saved data\n",
        "# If not, save the data into a directory\n",
        "\n",
        "#------------------------------------------------------------------------------\n",
        "DATA_SOURCE = \"yahoo\"\n",
        "COMPANY = \"TSLA\"\n",
        "split_by_date = True  # Set to False for random splitting\n",
        "test_size = 0.8\n",
        "\n",
        "\n",
        "# Load or download data\n",
        "if os.path.exists(\"stock_data.pkl\"):\n",
        "    with open(\"stock_data.pkl\", \"rb\") as f:\n",
        "        data = pickle.load(f)\n",
        "else:\n",
        "    # Changed the date from original and set to take User inputs.\n",
        "    TRAIN_START = input(\"Enter a start date in the format YYYY-MM-DD: \\n\")\n",
        "    TRAIN_END = input(\"Enter an end date in the format YYYY-MM-DD: \\n\")\n",
        "\n",
        "    data =  yf.download(COMPANY, start=TRAIN_START, end=TRAIN_END, progress=False)\n",
        "    data.dropna(inplace=True)\n",
        "\n",
        "    with open(\"stock_data.pkl\", \"wb\") as f:\n",
        "        pickle.dump(data, f)\n",
        "print(data)\n",
        "\n",
        "data.to_csv(\"stock_data.csv\")\n",
        "\n",
        "\n",
        "# Test data ratio\n",
        "\n",
        "# start = '2012-01-01', end='2017-01-01'\n",
        "\n",
        "# Changed the date from original and set to take User inputs.\n",
        "TRAIN_START = input(\"Enter a start date in the format YYYY-MM-DD: \\n\")\n",
        "TRAIN_END = input(\"Enter an end date in the format YYYY-MM-DD: \\n\")\n",
        "\n",
        "\n",
        "# yf.download(COMPANY, start = TRAIN_START, end=TRAIN_END)\n",
        "\n",
        "\n",
        "# For more details:\n",
        "# https://pandas.pydata.org/pandas-docs/stable/user_guide/dsintro.html\n",
        "#------------------------------------------------------------------------------\n",
        "# Prepare Data\n",
        "## To do:\n",
        "# 1) Check if data has been prepared before.\n",
        "# If so, load the saved data\n",
        "# If not, save the data into a directory\n",
        "# 2) Use a different price value eg. mid-point of Open & Close\n",
        "# 3) Change the Prediction days\n",
        "#------------------------------------------------------------------------------\n",
        "PRICE_VALUE = \"Close\"\n",
        "\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "# Note that, by default, feature_range=(0, 1). Thus, if you want a different\n",
        "# feature_range (min,max) then you'll need to specify it here\n",
        "scaled_data = scaler.fit_transform(data[PRICE_VALUE].values.reshape(-1, 1))\n",
        "scaler_filename = \"scaler.save\"\n",
        "joblib.dump = (scaler, scaler_filename)\n",
        "# To Load it: scaler = joblib.load(scaler_filename)\n",
        "\n",
        "# Flatten and normalise the data\n",
        "# First, we reshape a 1D array(n) to 2D array(n,1)\n",
        "# We have to do that because sklearn.preprocessing.fit_transform()\n",
        "# requires a 2D array\n",
        "# Here n == len(scaled_data)\n",
        "# Then, we scale the whole array to the range (0,1)\n",
        "# The parameter -1 allows (np.)reshape to figure out the array size n automatically\n",
        "# values.reshape(-1, 1)\n",
        "# https://stackoverflow.com/questions/18691084/what-does-1-mean-in-numpy-reshape'\n",
        "# When reshaping an array, the new shape must contain the same number of elements\n",
        "# as the old shape, meaning the products of the two shapes' dimensions must be equal.\n",
        "# When using a -1, the dimension corresponding to the -1 will be the product of\n",
        "# the dimensions of the original array divided by the product of the dimensions\n",
        "# given to reshape so as to maintain the same number of elements.\n",
        "\n",
        "# Number of days to look back to base the prediction\n",
        "PREDICTION_DAYS = 60 # Original\n",
        "\n",
        "# To store the training data\n",
        "x_train = []\n",
        "y_train = []\n",
        "\n",
        "scaled_data = scaled_data[:,0] # Turn the 2D array back to a 1D array\n",
        "# Prepare the data\n",
        "for x in range(PREDICTION_DAYS, len(scaled_data)):\n",
        "    x_train.append(scaled_data[x-PREDICTION_DAYS:x])\n",
        "    y_train.append(scaled_data[x])\n",
        "\n",
        "# Convert them into an array\n",
        "x_train, y_train = np.array(x_train), np.array(y_train)\n",
        "# Now, x_train is a 2D array(p,q) where p = len(scaled_data) - PREDICTION_DAYS\n",
        "# and q = PREDICTION_DAYS; while y_train is a 1D array(p)\n",
        "\n",
        "x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n",
        "# We now reshape x_train into a 3D array(p, q, 1); Note that x_train\n",
        "# is an array of p inputs with each input being a 2D array\n",
        "\n",
        "#------------------------------------------------------------------------------\n",
        "# Build the Model\n",
        "## TO DO:\n",
        "# 1) Check if data has been built before.\n",
        "# If so, load the saved data\n",
        "# If not, save the data into a directory\n",
        "# 2) Change the model to increase accuracy?\n",
        "#------------------------------------------------------------------------------\n",
        "# model = Sequential() # Basic neural network\n",
        "# # See: https://www.tensorflow.org/api_docs/python/tf/keras/Sequential\n",
        "# # for some useful examples\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define the parameters for the model\n",
        "\n",
        "# k_days = int(input('Enter the number of days you want to predict (for example 5 ): \\n')) # Number of days we want to predict.\n",
        "# sequence_length = int(input('Enter the number of sequence length(for example 60): \\n'))  # Length of input sequences\n",
        "# n_features = int(input ('Enter the value of the input layers(for example 1): \\n')) # Number of input features (e.g., for univariate time series data)\n",
        "# units = int(input('Enter the name of the LSTM Units in each layer(For example 64):\\n'))  # Number of LSTM units in each layer\n",
        "# n_layers = int(input('Enter the number of LSTM Layers(for example 2): \\n' ))  # Number of LSTM layers\n",
        "# dropout = float(input('Enter the drpoout rate (for example 0.6): \\n '))  # Dropout rate\n",
        "# loss = \"mean_absolute_error\"  # Loss function\n",
        "# optimizer = \"rmsprop\"  # Optimizer\n",
        "# bidirectional = True  # Whether to use bidirectional LSTM layers\n",
        "# layer_names = [LSTM, LSTM, Dense]\n",
        "# # Create the deep learning model\n",
        "\n",
        "\n",
        "# sequence_length = 60\n",
        "# n_features = 6  # Number of input features (e.g., open, high, low, close, adj_close, volume)\n",
        "# units = [64, 64, 64]\n",
        "# layer_names = [LSTM, LSTM, LSTM]\n",
        "\n",
        "# select = int(input(\"Press 1 for LSTM Model and press 2 for GRU Model:\"))\n",
        "\n",
        "# def create_gru_model(sequence_length, n_features, units=64, n_layers=2, dropout=0.3,\n",
        "#                                 loss=\"mean_absolute_error\", optimizer=\"rmsprop\", bidirectional=False):\n",
        "#             model = Sequential()\n",
        "\n",
        "#             for i in range(n_layers):\n",
        "#                 if i == 0:\n",
        "#                     # First layer\n",
        "#                     if bidirectional:\n",
        "#                         model.add(Bidirectional(GRU(units, return_sequences=True), batch_input_shape=(None, sequence_length, n_features)))\n",
        "#                     else:\n",
        "#                         model.add(GRU(units, return_sequences=True, batch_input_shape=(None, sequence_length, n_features)))\n",
        "#                 elif i == n_layers - 1:\n",
        "#                     # Last layer\n",
        "#                     if bidirectional:\n",
        "#                         model.add(Bidirectional(GRU(units, return_sequences=False)))\n",
        "#                     else:\n",
        "#                         model.add(GRU(units, return_sequences=False))\n",
        "#                 else:\n",
        "#                     # Hidden layers\n",
        "#                     if bidirectional:\n",
        "#                         model.add(Bidirectional(GRU(units, return_sequences=True)))\n",
        "#                     else:\n",
        "#                         model.add(GRU(units, return_sequences=True))\n",
        "\n",
        "#                 # Add dropout after each layer\n",
        "#                 model.add(Dropout(dropout))\n",
        "\n",
        "#             # Output layer\n",
        "#             model.add(Dense(1, activation=\"linear\"))\n",
        "\n",
        "#             # Compile the model\n",
        "#             model.compile(loss=loss, metrics=[\"mean_absolute_error\"], optimizer=optimizer)\n",
        "#             # Print a summary of the model's architecture\n",
        "#             model.summary()\n",
        "#             return model\n",
        "\n",
        "# if select==1:\n",
        "\n",
        "#     model = Sequential()\n",
        "\n",
        "#     for i in range(n_layers):\n",
        "#         if i == 0:\n",
        "#             # First layer\n",
        "#             if bidirectional:\n",
        "#                 model.add(Bidirectional(LSTM(units, return_sequences=True), batch_input_shape=(None, sequence_length, n_features)))\n",
        "#             else:\n",
        "#                 model.add(LSTM(units, return_sequences=True, batch_input_shape=(None, sequence_length, n_features)))\n",
        "#         elif i == n_layers - 1:\n",
        "#             # Last layer\n",
        "#             if bidirectional:\n",
        "#                 model.add(Bidirectional(LSTM(units, return_sequences=False)))\n",
        "#             else:\n",
        "#                 model.add(LSTM(units, return_sequences=False))\n",
        "#         else:\n",
        "#             # Hidden layers\n",
        "#             if bidirectional:\n",
        "#                 model.add(Bidirectional(LSTM(units, return_sequences=True)))\n",
        "#             else:\n",
        "#                 model.add(LSTM(units, return_sequences=True))\n",
        "\n",
        "#         # Add dropout after each layer\n",
        "#         model.add(Dropout(dropout))\n",
        "\n",
        "# # Output layer\n",
        "#         model.add(Dense(1, activation=\"linear\"))\n",
        "\n",
        "#         # Compile the model\n",
        "#         model.compile(loss=loss, metrics=[\"mean_absolute_error\"], optimizer=optimizer)\n",
        "\n",
        "#         # Print a summary of the model's architecture\n",
        "#         model.summary()\n",
        "# elif select==2 :\n",
        "#     model = create_gru_model(sequence_length, n_features, units=64, n_layers=2, dropout=0.3,\n",
        "#                                 loss=\"mean_absolute_error\", optimizer=\"rmsprop\", bidirectional=False)\n",
        "# else:\n",
        "#     print(\"Invalid selection. Please choose 1 for LSTM or 2 for GRU.\")\n",
        "#     exit()\n",
        "# k_days = 5\n",
        "# sequence_length = 60\n",
        "# n_features = 1\n",
        "# units = [64, 64, 64]\n",
        "# layer_names = [LSTM, LSTM, Dense]\n",
        "\n",
        "# def multistep_model(k_days, sequence_length, n_features, units, layer_names, n_layers=3, dropout_rate=0.3,\n",
        "#                     loss=\"mean_absolute_error\", optimizer=\"adam\", bidirectional=False):\n",
        "\n",
        "#     model = Sequential()\n",
        "\n",
        "#     for i in range(n_layers):\n",
        "#         if i == 0:\n",
        "#             # First layer\n",
        "#             if bidirectional:\n",
        "#                 model.add(Bidirectional(LSTM(units[i], return_sequences=True),\n",
        "#                                          input_shape=(sequence_length, n_features)))\n",
        "#             else:\n",
        "#                 model.add(LSTM(units[i], return_sequences=True,\n",
        "#                                input_shape=(sequence_length, n_features)))\n",
        "#         elif i == n_layers - 1:\n",
        "#             # Last layer\n",
        "#             if bidirectional:\n",
        "#                 model.add(Bidirectional(LSTM(units[i], return_sequences=False)))\n",
        "#             else:\n",
        "#                 model.add(LSTM(units[i], return_sequences=False))\n",
        "#         else:\n",
        "#             # Hidden layers\n",
        "#             if bidirectional:\n",
        "#                 model.add(Bidirectional(LSTM(units[i], return_sequences=True)))\n",
        "#             else:\n",
        "#                 model.add(LSTM(units[i], return_sequences=True))\n",
        "\n",
        "#         # Add dropout after each layer\n",
        "#         model.add(Dropout(dropout_rate))\n",
        "\n",
        "#     # Output layer\n",
        "#     model.add(Dense(k_days, activation=\"linear\"))\n",
        "\n",
        "#     # Compile the model\n",
        "#     model.compile(loss=loss, metrics=[\"mean_absolute_error\"], optimizer=optimizer)\n",
        "\n",
        "#     return model\n",
        "# model = multistep_model(k_days, sequence_length, n_features, units, layer_names)\n",
        "\n",
        "# def multivariate_stock_price_prediction(data, target_day, prediction_days=1, num_units=64, num_layers=2, dropout_rate=0.2, epochs=50, batch_size=32):\n",
        "#     # Extract features and target (closing price) from the data\n",
        "#     features = data[['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']].values\n",
        "#     target = data['Close'].values\n",
        "\n",
        "#     # Apply Min-Max scaling to features and target\n",
        "#     scaler = MinMaxScaler()\n",
        "#     features_scaled = scaler.fit_transform(features)\n",
        "#     target_scaled = scaler.fit_transform(target.reshape(-1, 1))\n",
        "\n",
        "#     # Create sequences of historical data for training\n",
        "#     X = []\n",
        "#     y = []\n",
        "#     for i in range(len(features_scaled) - target_day - prediction_days + 1):\n",
        "#         X.append(features_scaled[i:i+target_day])\n",
        "#         y.append(target_scaled[i+target_day:i+target_day+prediction_days])\n",
        "#     X = np.array(X)\n",
        "#     y = np.array(y)\n",
        "\n",
        "#     # Split data into training and testing sets\n",
        "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "#     # Build a sequential LSTM model\n",
        "#     model = Sequential()\n",
        "#     model.add(LSTM(units=num_units, return_sequences=True, input_shape=(target_day, features.shape[1])))\n",
        "#     for _ in range(num_layers - 2):\n",
        "#         model.add(LSTM(units=num_units, return_sequences=True))\n",
        "#         model.add(Dropout(dropout_rate))\n",
        "#     model.add(LSTM(units=num_units))\n",
        "#     model.add(Dropout(dropout_rate))\n",
        "#     model.add(Dense(prediction_days))  # Output layer\n",
        "\n",
        "#     # Compile the model\n",
        "#     model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "#     # Train the model\n",
        "#     model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test))\n",
        "\n",
        "#     # Make predictions for the specified day\n",
        "#     input_data = features_scaled[-target_day:]  # Use the most recent data\n",
        "#     input_data = np.reshape(input_data, (1, target_day, features.shape[1]))  # Reshape for prediction\n",
        "#     predicted_scaled = model.predict(input_data)\n",
        "\n",
        "#     # Inverse scaling to get the predicted closing price\n",
        "#     predicted_price = scaler.inverse_transform(predicted_scaled)\n",
        "\n",
        "#     return predicted_price\n",
        "\n",
        "# # Example usage:\n",
        "# # Predict the closing price for the next day (target_day=1)\n",
        "# predicted_price = multivariate_stock_price_prediction(data, target_day=1)\n",
        "# print(\"Predicted Closing Price:\", predicted_price)\n",
        "\n",
        "def multivariate_multistep_stock_price_prediction(data, target_day, prediction_days, num_units=64, num_layers=2, dropout_rate=0.2, epochs=50, batch_size=32):\n",
        "    # Extract features and target (closing price) from the data\n",
        "    features = data[['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']].values\n",
        "    target = data['Close'].values\n",
        "\n",
        "    # Apply Min-Max scaling to features and target\n",
        "    scaler = MinMaxScaler()\n",
        "    features_scaled = scaler.fit_transform(features)\n",
        "    target_scaled = scaler.fit_transform(target.reshape(-1, 1))\n",
        "\n",
        "    # Create sequences of historical data for training\n",
        "    X = []\n",
        "    y = []\n",
        "    for i in range(len(features_scaled) - target_day - prediction_days + 1):\n",
        "        X.append(features_scaled[i:i+target_day])\n",
        "        y.append(target_scaled[i+target_day:i+target_day+prediction_days])\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "\n",
        "    # Split data into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Build a sequential LSTM model\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(units=num_units, return_sequences=True, input_shape=(target_day, features.shape[1])))\n",
        "    for _ in range(num_layers - 2):\n",
        "        model.add(LSTM(units=num_units, return_sequences=True))\n",
        "        model.add(Dropout(dropout_rate))\n",
        "    model.add(LSTM(units=num_units))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(Dense(prediction_days))  # Output layer\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test))\n",
        "\n",
        "    # Make predictions for the specified day\n",
        "    input_data = features_scaled[-target_day:]  # Use the most recent data\n",
        "    input_data = np.reshape(input_data, (1, target_day, features.shape[1]))  # Reshape for prediction\n",
        "    predicted_scaled = model.predict(input_data)\n",
        "\n",
        "    # Inverse scaling to get the predicted closing prices\n",
        "    predicted_prices = scaler.inverse_transform(predicted_scaled)\n",
        "\n",
        "    return predicted_prices\n",
        "\n",
        "# Example usage:\n",
        "predicted_prices = multivariate_multistep_stock_price_prediction(data, target_day=1, prediction_days=5)\n",
        "print(\"Predicted Closing Prices:\", predicted_prices)\n",
        "\n",
        "# def multivariate_model(sequence_length, n_features, units, layer_names, n_layers=3, dropout_rate=0.3,\n",
        "#                        loss=\"mean_absolute_error\", optimizer=\"adam\", bidirectional=False):\n",
        "\n",
        "#     model = Sequential()\n",
        "\n",
        "#     for i in range(n_layers):\n",
        "#         if i == 0:\n",
        "#     # First layer\n",
        "#               if bidirectional:\n",
        "#                   model.add(Bidirectional(layer_names[i](units[i], return_sequences=True),\n",
        "#                                           input_shape=(sequence_length, n_features),))\n",
        "#               else:\n",
        "#                   model.add(layer_names[i](units[i], return_sequences=True,\n",
        "#                                           input_shape=(sequence_length, n_features),))\n",
        "\n",
        "#         elif i == n_layers - 1:\n",
        "#             # Last layer\n",
        "#             if bidirectional:\n",
        "#                 model.add(Bidirectional(layer_names[i](units[i], return_sequences=False)))\n",
        "#             else:\n",
        "#                 model.add(layer_names[i](units[i], return_sequences=False))\n",
        "#         else:\n",
        "#             # Hidden layers\n",
        "#             if bidirectional:\n",
        "#                 model.add(Bidirectional(layer_names[i](units[i], return_sequences=True)))\n",
        "#             else:\n",
        "#                 model.add(layer_names[i](units[i], return_sequences=True))\n",
        "\n",
        "#         # Add dropout after each layer\n",
        "#         model.add(Dropout(dropout_rate))\n",
        "\n",
        "#     # Output layer with LSTM units (no return_sequences for output)\n",
        "#     model.add(LSTM(n_features, activation=\"linear\"))\n",
        "\n",
        "#     # Compile the model\n",
        "#     model.compile(loss=loss, metrics=[\"mean_absolute_error\"], optimizer=optimizer)\n",
        "\n",
        "#     return model\n",
        "\n",
        "# model = multivariate_model(sequence_length, n_features, units, layer_names)\n",
        "\n",
        "\n",
        "# model.add(LSTM(units=50, return_sequences=True, input_shape=(x_train.shape[1], 1)))\n",
        "# # This is our first hidden layer which also spcifies an input layer.\n",
        "# # That's why we specify the input shape for this layer;\n",
        "# # i.e. the format of each training example\n",
        "# # The above would be equivalent to the following two lines of code:\n",
        "# # model.add(InputLayer(input_shape=(x_train.shape[1], 1)))\n",
        "# # model.add(LSTM(units=50, return_sequences=True))\n",
        "# # For som eadvances explanation of return_sequences:\n",
        "# # https://machinelearningmastery.com/return-sequences-and-return-states-for-lstms-in-keras/\n",
        "# # https://www.dlology.com/blog/how-to-use-return_state-or-return_sequences-in-keras/\n",
        "# # As explained there, for a stacked LSTM, you must set return_sequences=True\n",
        "# # when stacking LSTM layers so that the next LSTM layer has a\n",
        "# # three-dimensional sequence input.\n",
        "\n",
        "# # Finally, units specifies the number of nodes in this layer.\n",
        "# # This is one of the parameters you want to play with to see what number\n",
        "# # of units will give you better prediction quality (for your problem)\n",
        "\n",
        "# model.add(Dropout(0.2))\n",
        "# # The Dropout layer randomly sets input units to 0 with a frequency of\n",
        "# # rate (= 0.2 above) at each step during training time, which helps\n",
        "# # prevent overfitting (one of the major problems of ML).\n",
        "\n",
        "# model.add(LSTM(units=50, return_sequences=True))\n",
        "# # More on Stacked LSTM:\n",
        "# # https://machinelearningmastery.com/stacked-long-short-term-memory-networks/\n",
        "\n",
        "# model.add(Dropout(0.2))\n",
        "# model.add(LSTM(units=50))\n",
        "# model.add(Dropout(0.2))\n",
        "\n",
        "# model.add(Dense(units=1))\n",
        "# # Prediction of the next closing value of the stock price\n",
        "\n",
        "# # We compile the model by specify the parameters for the model\n",
        "# # See lecture Week 6 (COS30018)\n",
        "# model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "# # The optimizer and loss are two important parameters when building an\n",
        "# # ANN model. Choosing a different optimizer/loss can affect the prediction\n",
        "# # quality significantly. You should try other settings to learn; e.g.\n",
        "\n",
        "# # optimizer='rmsprop'/'sgd'/'adadelta'/...\n",
        "# # loss='mean_absolute_error'/'huber_loss'/'cosine_similarity'/...\n",
        "\n",
        "# # Now we are going to train this model with our training data\n",
        "# # (x_train, y_train)\n",
        "# model.fit(x_train, y_train, epochs=25, batch_size=32)\n",
        "# Other parameters to consider: How many rounds(epochs) are we going to\n",
        "# train our model? Typically, the more the better, but be careful about\n",
        "# overfitting!\n",
        "# What about batch_size? Well, again, please refer to\n",
        "# Lecture Week 6 (COS30018): If you update your model for each and every\n",
        "# input sample, then there are potentially 2 issues: 1. If you training\n",
        "# data is very big (billions of input samples) then it will take VERY long;\n",
        "# 2. Each and every input can immediately makes changes to your model\n",
        "# (a souce of overfitting). Thus, we do this in batches: We'll look at\n",
        "# the aggreated errors/losses from a batch of, say, 32 input samples\n",
        "# and update our model based on this aggregated loss.\n",
        "\n",
        "# TO DO:\n",
        "# Save the model and reload it\n",
        "# Sometimes, it takes a lot of effort to train your model (again, look at\n",
        "# a training data with billions of input samples). Thus, after spending so\n",
        "# much computing power to train your model, you may want to save it so that\n",
        "# in the future, when you want to make the prediction, you only need to load\n",
        "# your pre-trained model and run it on the new input for which the prediction\n",
        "# need to be made.\n",
        "\n",
        "#------------------------------------------------------------------------------\n",
        "# Test the model accuracy on existing data\n",
        "#------------------------------------------------------------------------------\n",
        "# Load the test data\n",
        "TEST_START = '2020-01-02'\n",
        "TEST_END = '2022-12-31'\n",
        "\n",
        "test_data = yf.download(COMPANY, start=TRAIN_START, end=TRAIN_END, progress=False)\n",
        "\n",
        "# The above bug is the reason for the following line of code\n",
        "test_data = test_data[1:]\n",
        "\n",
        "actual_prices = test_data[PRICE_VALUE].values\n",
        "\n",
        "total_dataset = pd.concat((data[PRICE_VALUE], test_data[PRICE_VALUE]), axis=0)\n",
        "\n",
        "model_inputs = total_dataset[len(total_dataset) - len(test_data) - PREDICTION_DAYS:].values\n",
        "# We need to do the above because to predict the closing price of the fisrt\n",
        "# PREDICTION_DAYS of the test period [TEST_START, TEST_END], we'll need the\n",
        "# data from the training period\n",
        "\n",
        "model_inputs = model_inputs.reshape(-1, 1)\n",
        "# TO DO: Explain the above line\n",
        "\n",
        "model_inputs = scaler.transform(model_inputs)\n",
        "# We again normalize our closing price data to fit them into the range (0,1)\n",
        "# using the same scaler used above\n",
        "# However, there may be a problem: scaler was computed on the basis of\n",
        "# the Max/Min of the stock price for the period [TRAIN_START, TRAIN_END],\n",
        "# but there may be a lower/higher price during the test period\n",
        "# [TEST_START, TEST_END]. That can lead to out-of-bound values (negative and\n",
        "# greater than one)\n",
        "# We'll call this ISSUE #2\n",
        "\n",
        "# TO DO: Generally, there is a better way to process the data so that we\n",
        "# can use part of it for training and the rest for testing. You need to\n",
        "# implement such a way\n",
        "\n",
        "#------------------------------------------------------------------------------\n",
        "# Make predictions on test data\n",
        "#------------------------------------------------------------------------------\n",
        "x_test = []\n",
        "for x in range(PREDICTION_DAYS, len(model_inputs)):\n",
        "    x_test.append(model_inputs[x - PREDICTION_DAYS:x, 0])\n",
        "\n",
        "x_test = np.array(x_test)\n",
        "x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))\n",
        "# TO DO: Explain the above 5 lines\n",
        "\n",
        "predicted_prices = model.predict(x_test)\n",
        "predicted_prices = scaler.inverse_transform(predicted_prices)\n",
        "# Clearly, as we transform our data into the normalized range (0,1),\n",
        "# we now need to reverse this transformation\n",
        "#------------------------------------------------------------------------------\n",
        "# Plot the test predictions\n",
        "## To do:\n",
        "# 1) Candle stick charts\n",
        "# 2) Chart showing High & Lows of the day\n",
        "# 3) Show chart of next few days (predicted)\n",
        "#------------------------------------------------------------------------------\n",
        "\n",
        "# creating a variable and storing the data\n",
        "# tsla_df = pd.read_csv('stock_data.csv', index_col= 0 ,parse_dates= True)\n",
        "# dt_range = pd.date_range(start = TRAIN_START, end = TRAIN_END)\n",
        "# tsla_df = tsla_df[tsla_df.index.isin(dt_range)]\n",
        "# tsla_df.head()\n",
        "# # Making the Candle stick Chart\n",
        "# fplt.plot(\n",
        "#             tsla_df,\n",
        "#             type='candle',\n",
        "#             style='charles',\n",
        "#             title='TSLA, CandleStickChart',\n",
        "#             ylabel='Price ($)'\n",
        "#         )\n",
        "# # Values for Individual Candlesticks or n trading days\n",
        "# candlestick = go.Candlestick(\n",
        "#                             x=tsla_df.index,\n",
        "#                             open=tsla_df['Open'],\n",
        "#                             high=tsla_df['High'],\n",
        "#                             low=tsla_df['Low'],\n",
        "#                             close=tsla_df['Close']\n",
        "#                             )\n",
        "\n",
        "# fig = go.Figure(data=[candlestick])\n",
        "\n",
        "# fig.update_layout(\n",
        "#     width=800, height=600,\n",
        "#     title=\"TSLA STOCK\",\n",
        "#     yaxis_title='TSLA Stock'\n",
        "# )\n",
        "\n",
        "# fig.show()\n",
        "# # Making the boxplot Chart\n",
        "\n",
        "# # Load data from CSV file into a DataFrame\n",
        "# csv_file_path = 'stock_data.csv'\n",
        "# df = pd.read_csv(csv_file_path)\n",
        "\n",
        "# df['Date'] = pd.to_datetime(df['Date'])  # Convert 'Date' column to datetime\n",
        "# df.sort_values(by='Date', inplace=True)\n",
        "# df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# # Slice the last 30 days of data\n",
        "# last_30_days_data = df.tail(30)\n",
        "\n",
        "# # Save the sliced data to a new CSV file\n",
        "# output_csv_path = \"30daysdata.csv\"\n",
        "# last_30_days_data.to_csv(output_csv_path, index=False)\n",
        "\n",
        "# # Set the company name and rolling window\n",
        "# company_name = \"TSLA\"\n",
        "# rolling_days = 30  # Adjust this value as needed\n",
        "# df = pd.read_csv(output_csv_path)\n",
        "# # Create the figure using Plotly Express\n",
        "# fig = px.box(df,\n",
        "#              x='Date',  # Assuming 'Date' is the column name for dates\n",
        "#              y=['High', 'Low', 'Close', 'Open'],\n",
        "#              title=f\"{company_name} Prices Boxed over {rolling_days} Day Window\",\n",
        "#              labels={'x': 'Dates', 'y': 'Price ($)'})\n",
        "\n",
        "# # Display the figure\n",
        "# fig.show()\n",
        "#------------------------------------------------------------------------------\n",
        "# Predict next day\n",
        "#------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "real_data = [model_inputs[len(model_inputs) - PREDICTION_DAYS:, 0]]\n",
        "real_data = np.array(real_data)\n",
        "real_data = np.reshape(real_data, (real_data.shape[0], real_data.shape[1], 1))\n",
        "\n",
        "prediction = model.predict(real_data)\n",
        "prediction = scaler.inverse_transform(prediction)\n",
        "print(f\"Prediction: {prediction}\")\n",
        "\n",
        "# A few concluding remarks here:\n",
        "# 1. The predictor is quite bad, especially if you look at the next day\n",
        "# prediction, it missed the actual price by about 10%-13%\n",
        "# Can you find the reason?\n",
        "# 2. The code base at\n",
        "# https://github.com/x4nth055/pythoncode-tutorials/tree/master/machine-learning/stock-prediction\n",
        "# gives a much better prediction. Even though on the surface, it didn't seem\n",
        "# to be a big difference (both use Stacked LSTM)\n",
        "# Again, can you explain it?\n",
        "# A more advanced and quite different technique use CNN to analyse the images\n",
        "# of the stock price changes to detect some patterns with the trend of\n",
        "# the stock price:\n",
        "# https://github.com/jason887/Using-Deep-Learning-Neural-Networks-and-Candlestick-Chart-Representation-to-Predict-Stock-Market\n",
        "# Can you combine these different techniques for a better prediction??\n",
        "# Add the following section for data splitting flexibility\n",
        "\n",
        "if split_by_date:\n",
        "    # Split data by date\n",
        "    train_samples = int((1 - test_size) * len(x_train))\n",
        "    x_train_final = x_train[:train_samples]\n",
        "    y_train_final = y_train[:train_samples]\n",
        "    x_test_final = x_train[train_samples:]\n",
        "    y_test_final = y_train[train_samples:]\n",
        "else:\n",
        "    # Split data randomly\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    x_train_final, x_test_final, y_train_final, y_test_final = train_test_split(\n",
        "        x_train, y_train, test_size=test_size, shuffle=True\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_3ELLfjeilD",
        "outputId": "cc9cf2bf-9d4a-48d5-89d0-963cb4937dfd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                  Open        High         Low       Close   Adj Close  \\\n",
            "Date                                                                     \n",
            "2020-01-02   28.299999   28.713333   28.114000   28.684000   28.684000   \n",
            "2020-01-03   29.366667   30.266666   29.128000   29.534000   29.534000   \n",
            "2020-01-06   29.364668   30.104000   29.333332   30.102667   30.102667   \n",
            "2020-01-07   30.760000   31.441999   30.224001   31.270666   31.270666   \n",
            "2020-01-08   31.580000   33.232666   31.215334   32.809334   32.809334   \n",
            "...                ...         ...         ...         ...         ...   \n",
            "2021-12-27  357.890015  372.333344  356.906677  364.646667  364.646667   \n",
            "2021-12-28  369.829987  373.000000  359.473328  362.823334  362.823334   \n",
            "2021-12-29  366.213318  368.000000  354.713318  362.063324  362.063324   \n",
            "2021-12-30  353.776672  365.183319  351.049988  356.779999  356.779999   \n",
            "2021-12-31  357.813324  360.666656  351.529999  352.260010  352.260010   \n",
            "\n",
            "               Volume  \n",
            "Date                   \n",
            "2020-01-02  142981500  \n",
            "2020-01-03  266677500  \n",
            "2020-01-06  151995000  \n",
            "2020-01-07  268231500  \n",
            "2020-01-08  467164500  \n",
            "...               ...  \n",
            "2021-12-27   71145900  \n",
            "2021-12-28   60324000  \n",
            "2021-12-29   56154000  \n",
            "2021-12-30   47040900  \n",
            "2021-12-31   40733700  \n",
            "\n",
            "[505 rows x 6 columns]\n",
            "Enter a start date in the format YYYY-MM-DD: \n",
            "2020-01-01\n",
            "Enter an end date in the format YYYY-MM-DD: \n",
            "2022-01-01\n",
            "Epoch 1/50\n",
            "13/13 [==============================] - 7s 166ms/step - loss: 0.2014 - val_loss: 0.1828\n",
            "Epoch 2/50\n",
            "13/13 [==============================] - 0s 19ms/step - loss: 0.1453 - val_loss: 0.1108\n",
            "Epoch 3/50\n",
            "13/13 [==============================] - 0s 16ms/step - loss: 0.0680 - val_loss: 0.0277\n",
            "Epoch 4/50\n",
            "13/13 [==============================] - 0s 20ms/step - loss: 0.0160 - val_loss: 0.0137\n",
            "Epoch 5/50\n",
            "13/13 [==============================] - 0s 11ms/step - loss: 0.0121 - val_loss: 0.0075\n",
            "Epoch 6/50\n",
            "13/13 [==============================] - 0s 10ms/step - loss: 0.0085 - val_loss: 0.0063\n",
            "Epoch 7/50\n",
            "13/13 [==============================] - 0s 11ms/step - loss: 0.0068 - val_loss: 0.0043\n",
            "Epoch 8/50\n",
            "13/13 [==============================] - 0s 11ms/step - loss: 0.0058 - val_loss: 0.0034\n",
            "Epoch 9/50\n",
            "13/13 [==============================] - 0s 14ms/step - loss: 0.0050 - val_loss: 0.0027\n",
            "Epoch 10/50\n",
            "13/13 [==============================] - 0s 11ms/step - loss: 0.0048 - val_loss: 0.0024\n",
            "Epoch 11/50\n",
            "13/13 [==============================] - 0s 12ms/step - loss: 0.0044 - val_loss: 0.0022\n",
            "Epoch 12/50\n",
            "13/13 [==============================] - 0s 10ms/step - loss: 0.0040 - val_loss: 0.0021\n",
            "Epoch 13/50\n",
            "13/13 [==============================] - 0s 12ms/step - loss: 0.0038 - val_loss: 0.0020\n",
            "Epoch 14/50\n",
            "13/13 [==============================] - 0s 11ms/step - loss: 0.0040 - val_loss: 0.0020\n",
            "Epoch 15/50\n",
            "13/13 [==============================] - 0s 13ms/step - loss: 0.0040 - val_loss: 0.0020\n",
            "Epoch 16/50\n",
            "13/13 [==============================] - 0s 11ms/step - loss: 0.0038 - val_loss: 0.0019\n",
            "Epoch 17/50\n",
            "13/13 [==============================] - 0s 11ms/step - loss: 0.0037 - val_loss: 0.0019\n",
            "Epoch 18/50\n",
            "13/13 [==============================] - 0s 11ms/step - loss: 0.0038 - val_loss: 0.0018\n",
            "Epoch 19/50\n",
            "13/13 [==============================] - 0s 9ms/step - loss: 0.0036 - val_loss: 0.0018\n",
            "Epoch 20/50\n",
            "13/13 [==============================] - 0s 10ms/step - loss: 0.0041 - val_loss: 0.0018\n",
            "Epoch 21/50\n",
            "13/13 [==============================] - 0s 10ms/step - loss: 0.0033 - val_loss: 0.0017\n",
            "Epoch 22/50\n",
            "13/13 [==============================] - 0s 11ms/step - loss: 0.0036 - val_loss: 0.0017\n",
            "Epoch 23/50\n",
            "13/13 [==============================] - 0s 10ms/step - loss: 0.0034 - val_loss: 0.0017\n",
            "Epoch 24/50\n",
            "13/13 [==============================] - 0s 10ms/step - loss: 0.0033 - val_loss: 0.0017\n",
            "Epoch 25/50\n",
            "13/13 [==============================] - 0s 9ms/step - loss: 0.0033 - val_loss: 0.0016\n",
            "Epoch 26/50\n",
            "13/13 [==============================] - 0s 10ms/step - loss: 0.0031 - val_loss: 0.0016\n",
            "Epoch 27/50\n",
            "13/13 [==============================] - 0s 14ms/step - loss: 0.0032 - val_loss: 0.0016\n",
            "Epoch 28/50\n",
            "13/13 [==============================] - 0s 12ms/step - loss: 0.0033 - val_loss: 0.0016\n",
            "Epoch 29/50\n",
            "13/13 [==============================] - 0s 12ms/step - loss: 0.0032 - val_loss: 0.0016\n",
            "Epoch 30/50\n",
            "13/13 [==============================] - 0s 11ms/step - loss: 0.0029 - val_loss: 0.0015\n",
            "Epoch 31/50\n",
            "13/13 [==============================] - 0s 11ms/step - loss: 0.0028 - val_loss: 0.0016\n",
            "Epoch 32/50\n",
            "13/13 [==============================] - 0s 10ms/step - loss: 0.0030 - val_loss: 0.0015\n",
            "Epoch 33/50\n",
            "13/13 [==============================] - 0s 11ms/step - loss: 0.0032 - val_loss: 0.0015\n",
            "Epoch 34/50\n",
            "13/13 [==============================] - 0s 10ms/step - loss: 0.0029 - val_loss: 0.0015\n",
            "Epoch 35/50\n",
            "13/13 [==============================] - 0s 13ms/step - loss: 0.0030 - val_loss: 0.0015\n",
            "Epoch 36/50\n",
            "13/13 [==============================] - 0s 12ms/step - loss: 0.0029 - val_loss: 0.0015\n",
            "Epoch 37/50\n",
            "13/13 [==============================] - 0s 11ms/step - loss: 0.0028 - val_loss: 0.0015\n",
            "Epoch 38/50\n",
            "13/13 [==============================] - 0s 10ms/step - loss: 0.0030 - val_loss: 0.0014\n",
            "Epoch 39/50\n",
            "13/13 [==============================] - 0s 10ms/step - loss: 0.0028 - val_loss: 0.0014\n",
            "Epoch 40/50\n",
            "13/13 [==============================] - 0s 9ms/step - loss: 0.0029 - val_loss: 0.0014\n",
            "Epoch 41/50\n",
            "13/13 [==============================] - 0s 9ms/step - loss: 0.0029 - val_loss: 0.0014\n",
            "Epoch 42/50\n",
            "13/13 [==============================] - 0s 11ms/step - loss: 0.0025 - val_loss: 0.0014\n",
            "Epoch 43/50\n",
            "13/13 [==============================] - 0s 11ms/step - loss: 0.0029 - val_loss: 0.0014\n",
            "Epoch 44/50\n",
            "13/13 [==============================] - 0s 9ms/step - loss: 0.0030 - val_loss: 0.0014\n",
            "Epoch 45/50\n",
            "13/13 [==============================] - 0s 10ms/step - loss: 0.0025 - val_loss: 0.0014\n",
            "Epoch 46/50\n",
            "13/13 [==============================] - 0s 8ms/step - loss: 0.0026 - val_loss: 0.0014\n",
            "Epoch 47/50\n",
            "13/13 [==============================] - 0s 10ms/step - loss: 0.0027 - val_loss: 0.0014\n",
            "Epoch 48/50\n",
            "13/13 [==============================] - 0s 11ms/step - loss: 0.0027 - val_loss: 0.0015\n",
            "Epoch 49/50\n",
            "13/13 [==============================] - 0s 11ms/step - loss: 0.0031 - val_loss: 0.0015\n",
            "Epoch 50/50\n",
            "13/13 [==============================] - 0s 11ms/step - loss: 0.0026 - val_loss: 0.0014\n",
            "1/1 [==============================] - 1s 904ms/step\n",
            "Predicted Closing Prices: [[354.9966  355.63614 356.92078 353.49313 351.8985 ]]\n",
            "16/16 [==============================] - 1s 31ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "Prediction: [[13.893775 50.242382 40.165092 12.501049 17.723415]]\n"
          ]
        }
      ]
    }
  ]
}